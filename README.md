# nlp_geonames_simantic
Comparison of arbitrary geo names with unified geonames (for internal use by the Career Center).

# Cопоставление геоназваний с унифицированными именами geonames

# Задача
- Создать решение для подбора наиболее подходящих названий с geonames.
Например Ереван -> Yerevan
- На примере РФ и стран наиболее популярных для релокации - Беларусь, Армения,
Казахстан, Кыргызстан, Турция, Сербия. Города с населением от 15000 человек (с
возможностью масштабирования на сервере заказчика)
- Возвращаемые поля *geonameid, name, region, country, cosine similarity*
- формат данных на выходе: список словарей, например [{dict_1}, {dict_2}, …. {dict_n}], где словарь - одна запись с указанными полями

# Цель
Сопоставление произвольных гео названий с унифицированными именами geonames для внутреннего использования Карьерным центром.

# План работы

- Подготовить данные к обучению.
 - Метод векторизации слова CountVectorizer();
 - Метод использования готовой предобученной модели из SentenceTransformers;
 - Метод использования своей предобученной модели из transformers.

# Описание данных

Используемые таблицы с geonames:

- admin1CodesASCII
- alternateNamesV2
- cities15000
- countryInfo
- при необходимости любые другие открытые данные
- таблицы geonames можно скачать здесь http://download.geonames.org/export/dump/
- Тестовый датасет: https://disk.yandex.ru/d/wC296Rj3Yso2AQ

# Используемые библиотеки

*pandas, numpy, sklearn, tqdm, torch, transformers, SentenceTransformer, os*

# Применение

* В папке `mbart_finetuning` - пример обучения модели facebook/mbart-large-50-many-to-many-mmt

        * `augmentation_add_typo.py` - скрипт для создания случайной опечатки в слове (6 языков):

              - get_random_letter(language_code) - Функция принимает один из 6 языков и возвращает букву из соответствующего алфавита
              - add_typo(word, country) - Функция принимает слово и страну для создания случайной опечатки в слове
        
        * `city_dataset.py` - Создает тренировочный и валидационный датасет после разделения данных на выборки

* `geonames` - основной блокнот, с примерами использования сприптов и функций

* `geonames__test_file` - файл с результатами на тестовых данных

* `count_vec_mod.py` - скрипт для реализации подбора наиболее подходящих названий с geonames с помощию векторного представления слова

* `semantic_mod.py` - скрипт для реализации подбора наиболее подходящих названий с geonames с помощию предобученной модели из SentenceTransformers

              - get_sim(city, names, embeddings, model, table, top=5)- функция для нахождения cosine-similarity
              - get_mahalanobis(city, names, embeddings, model, table, cov_matrix, top=5) - функция для нахождения расстояния mahalanobis

* `generation_semantic_mod.py` - скрипт для реализации подбора наиболее подходящих названий с geonames с помощию предобученной модели из  transformers

# Обучение

Модель facebook/mbart-large-50-many-to-many-mmt из HaggingFace была дообучена на списке альтернативных имен с аугментациями в виде случайной опечатки для каждого наименования. 

Модель обучена на 8 эпохах, с разделением на два gpu T4x2 Max 14.8 Gb.

При оценке качества модели тем же способом, что и в файле `geonames _test_file`, показала результаты `'score' = 0.8276` чуть хуже, чем модель на 4 эпохах с аугментациями `'score' = 0.8448` (которая и была добавлена на hugginFace и использовалась для демонстрации результатов на тестовой выборке). 

Рекомендуется дообучить модель на больших эпохах, чтобы выбрать наиболее оптимальный. (по функции Loss, которая на8 эпохах не вышла на минимум).

# Выводы

Очень важно сделать *правильную предобработку* исходных слов, в том числе добавить дополнительные слова с другими аугментациями удаление символов, перестановка символов, семантические аугментации, расширение синонимами.

Все эти методы очень сильно зависят от столбца с альтернативными названиями. То есть данный словарь должен пополняться, если встречаются новые слова. (но нужно различать, насколько данное слово подходит в качестве альтернативного названия).

Как можно было бы улучишь качество:

- Качество данных на входе: оценить еще раз те слова, которые модель не смогла предугадать, и добавить дополнительные аугментации.
- Анализ ошибок: к примеру модель сильно ошибается, когда названия городов исковерканы значительно, то есть более 3-4 ошибок в слове, зато почти идеально работает с меньшим количеством ошибок в слове.
- Увеличить количество эпох в обучении и выбрать оптимальную модель.



